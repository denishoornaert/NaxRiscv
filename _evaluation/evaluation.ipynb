{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelimnary Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- One Time Setup ---\n",
    "import os\n",
    "\n",
    "# Ensure setup is only done once\n",
    "try:\n",
    "    if SETUP_IS_DONE:\n",
    "        print(\"Skipping Setup\")\n",
    "except:\n",
    "    # Perform Setup\n",
    "    \n",
    "    # Path to NaxRiscv repository\n",
    "    path_results = os.getcwd()\n",
    "    path_nax_repo = os.getcwd() + \"/../\"\n",
    "    path_benchmarker = path_nax_repo + \"/ext/NaxSoftware/baremetal/benchmarker/\"\n",
    "    from pathlib import Path\n",
    "    path_errors = path_results + \"/errors/\"\n",
    "    Path(path_errors).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Fix because /bin is automatically added to the start of the PATH by default & messes up some verilator stuff\n",
    "    # TODO: Is this really required?\n",
    "    os.environ[\"PATH\"] = os.environ[\"PATH\"].split(\":\",1)[1]\n",
    "\n",
    "SETUP_IS_DONE = True\n",
    "\n",
    "# Presets to use for benchmark configurations\n",
    "with_all_cores = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "with_some_cores = [0, 3, 7]\n",
    "with_min_cores = [0]\n",
    "\n",
    "with_all_prios = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "with_some_prios = [0, 1, 2]\n",
    "with_min_prios = [0]\n",
    "with_no_prios = [\"none\"]\n",
    "\n",
    "with_all_coworkers = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "with_some_coworkers = [0, 1, 4, 7]\n",
    "with_min_coworkers = [0, 7]\n",
    "\n",
    "with_standard_memconf = [ # (mem-resp-time, core-max-trans, down-pending-max=4, probe-count=1)\n",
    "    (0, 2, 4, 1), \n",
    "    (0, 4, 4, 1), \n",
    "    (70, 2, 4, 1), \n",
    "    (281, 4, 4, 1),\n",
    "]\n",
    "with_asic_memconf = [ # (mem-resp-time, core-max-trans, down-pending-max=4, probe-count=1)\n",
    "    (281, 4, 4, 1),\n",
    "]\n",
    "with_fpga_memconf = [ # (mem-resp-time, core-max-trans, down-pending-max=4, probe-count=1)\n",
    "    (70, 2, 4, 1),\n",
    "]\n",
    "with_debug_memconf = [ # (mem-resp-time, core-max-trans, down-pending-max=4, probe-count=1)\n",
    "    (0, 4, 4, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark is expected to finish in less than 2 h 40 min\n"
     ]
    }
   ],
   "source": [
    "# ------------ CONFIG ---------------\n",
    "\n",
    "bench_configs = [ # (\"BENCH_LATENCY\" or \"BENCH_BANDWIDTH\", cores, prios, coworkers, memconf)\n",
    "    # Q0\n",
    "    (\"BENCH_BANDWIDTH\", with_min_prios, with_no_prios, with_some_coworkers, [ \n",
    "        # (mem-resp-time, core-max-trans, down-pending-max=4, probe-count=1)\n",
    "        (70, 4, 4, 1),\n",
    "        (281, 4, 4, 1),\n",
    "        (281, 2, 4, 1),\n",
    "        (281, 8, 4, 1),\n",
    "        (281, 4, 2, 1),\n",
    "        (281, 4, 8, 1),\n",
    "        (281, 4, 4, 4),\n",
    "        (281, 4, 4, 8),\n",
    "    ]),\n",
    "\n",
    "    # Q1\n",
    "    # (\"BENCH_BANDWIDTH\", [ 0 ], with_no_prios, with_some_coworkers, with_asic_memconf),\n",
    "    \n",
    "    # Q3\n",
    "    # (\"BENCH_LATENCY\", [ 0 ], with_no_prios, with_some_coworkers, with_debug_memconf),\n",
    "    \n",
    "    ]\n",
    "\n",
    "bench_name = \"var\" # BENCH_NAME\n",
    "\n",
    "# Timeout in seconds\n",
    "timeout = 300 # 5 min\n",
    "\n",
    "# ------------ ------ ---------------\n",
    "\n",
    "def get_bench_num():\n",
    "    bench_list = []\n",
    "    for (bench, cores, prios, workers, memconf) in bench_configs:\n",
    "        for main_core in cores:\n",
    "            for main_prio in prios:\n",
    "                for worker_num in workers:\n",
    "                    for (mem_resp_time, core_max_transactions, down_pending_max, probe_count) in memconf:\n",
    "                        bench_id = f\"{bench_name}_{bench}_{main_core}main_{main_prio}prio_{worker_num}cores_{mem_resp_time}resp_{core_max_transactions}trans_{down_pending_max}dp_{probe_count}probes\"\n",
    "                        if bench_id in bench_list:\n",
    "                            continue\n",
    "                        bench_list.append(bench_id)\n",
    "    return len(bench_list)\n",
    "\n",
    "print(f\"Benchmark is expected to finish in less than {int(get_bench_num() * timeout // 3600 )} h {int((get_bench_num() * timeout) % 3600 // 60)} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmarks...\n",
      "Benchmark is expected to finish before February 28 16:40\n",
      "  Compiling BENCH_BANDWIDTH for core 0 with prio none and 0 coworkers\n",
      "    Running benchmark [1/2] with memconf=(0, 4, 4, 1), max 10 min remaining\n",
      "    [success] Total time: 18 s, completed Feb 28, 2024, 4:31:18 PM\n",
      "  Compiling BENCH_BANDWIDTH for core 0 with prio none and 7 coworkers\n",
      "    Running benchmark [2/2] with memconf=(0, 4, 4, 1), max 5 min remaining\n",
      "    [success] Total time: 30 s, completed Feb 28, 2024, 4:31:51 PM\n",
      "Benchmark completed in 0 min 54.47 s\n",
      "Result saved to /home/julianpritzi/NaxRiscv/_evaluation/benchmark_all_results.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "import datetime\n",
    "import shutil\n",
    "import signal\n",
    "from datetime import datetime, timedelta\n",
    "        \n",
    "print(\"Starting benchmarks...\")\n",
    "num_bench = get_bench_num()\n",
    "bench_ctr = 0\n",
    "start_time = time.time()\n",
    "finished_before = (datetime.now() + timedelta(seconds=(timeout * num_bench))).strftime(\"%B %d %H:%M\")\n",
    "print(f\"Benchmark is expected to finish before {finished_before}\")\n",
    "\n",
    "completed_ids = []\n",
    "data_lines = []\n",
    "data_header = [\"Benchmark\", \"MainCore\", \"MainPrio\", \"WorkerNum\", \"MemMaxTransaction\", \"MemResponseTime\", \"DownPendingMax\", \"ProbeCount\", \"BufferSize\", \"Cycles\"]\n",
    "for (bench, cores, prios, workers, memconf) in bench_configs:\n",
    "    for main_core in cores:\n",
    "        for main_prio in prios:\n",
    "            for worker_num in workers:\n",
    "                \n",
    "                # Compile the benchmarker\n",
    "                print(f\"  Compiling {bench} for core {main_core} with prio {main_prio} and {worker_num} coworkers\")\n",
    "                os.chdir(path_benchmarker)\n",
    "                subprocess.run([\"make\", \"clean\"], check=True)\n",
    "                subprocess.run([\"make\", f\"WORKER_NUM={worker_num}\", f\"MAIN_CORE={main_core}\", f\"MAIN_PRIO={main_prio}\", f\"BENCH={bench}\"], check=True, capture_output=True)\n",
    "                \n",
    "                # Run with different configurations\n",
    "                for (mem_resp_time, core_max_transactions, down_pending_max, probe_count) in memconf:\n",
    "                    max_remaining_time = (num_bench - bench_ctr) * timeout\n",
    "                    bench_id = f\"{bench_name}_{bench}_{main_core}main_{main_prio}prio_{worker_num}cores_{mem_resp_time}resp_{core_max_transactions}trans_{down_pending_max}dp_{probe_count}probes\"\n",
    "                    \n",
    "                    if bench_id in completed_ids:\n",
    "                        print(f\"    Skipping benchmark with memconf={(mem_resp_time, core_max_transactions, down_pending_max, probe_count)}, already done\")\n",
    "                        continue\n",
    "                    \n",
    "                    bench_ctr += 1\n",
    "                    print(f\"    Running benchmark [{bench_ctr}/{num_bench}] with memconf={(mem_resp_time, core_max_transactions, down_pending_max, probe_count)}, max {int(max_remaining_time // 60)} min remaining\")\n",
    "                    completed_ids.append(bench_id)\n",
    "                    \n",
    "                    proc = None\n",
    "                    try: \n",
    "                        os.chdir(path_nax_repo)\n",
    "                        proc = subprocess.Popen([\"sbt\", f\"runMain naxriscv.platform.rtccidemo.SocSim \\\n",
    "                                                --load-elf {path_benchmarker}/build/benchmarker.elf \\\n",
    "                                                --xlen 32 \\\n",
    "                                                --nax-count 8 \\\n",
    "                                                --core-max-trans {core_max_transactions} \\\n",
    "                                                --mem-resp-time {mem_resp_time} \\\n",
    "                                                --down-pending-max {down_pending_max} \\\n",
    "                                                --probe-count {probe_count} \\\n",
    "                                                --no-rvls \\\n",
    "                                                --trace\"], stdout=subprocess.PIPE, preexec_fn=os.setsid)\n",
    "                        (stdout_data, stderr_data) = proc.communicate(timeout=timeout)\n",
    "                        \n",
    "                        file_name = f\"bench_{bench_id}\"\n",
    "                        try: # Save Waveform\n",
    "                            shutil.copy(path_nax_repo + \"/simWorkspace/SocDemo/test.fst\",\n",
    "                                        path_nax_repo + f\"/waveforms/bench/{file_name}.fst\")\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        if proc.returncode == 0:\n",
    "                            # Read Result\n",
    "                            buffer_size = re.search(rb\"BufferSize: [^=]*= (\\d*)\", stdout_data).group(1)\n",
    "                            cycles = re.search(rb\"Cycles: [^=]*= (\\d*)\", stdout_data).group(1)\n",
    "                            print(\"    \" + stdout_data.splitlines()[-1].decode(\"utf-8\"))\n",
    "                            data_lines += [[bench, main_core, main_prio, worker_num, core_max_transactions, mem_resp_time, down_pending_max, probe_count, int(buffer_size), int(cycles)]]\n",
    "                            \n",
    "                            # Save Progress\n",
    "                            df = pd.DataFrame(data_lines, columns=data_header)\n",
    "                            df.to_csv(path_results + f\"/benchmark_{bench_name}_results_partial.csv\", index=False)\n",
    "                        else:\n",
    "                            print(f\"    [ERROR] simulation crashed, error dumped in errors/bench_{bench_name}_{mem_resp_time}resp_{core_max_transactions}trans_{down_pending_max}dp_{probe_count}probes_{worker_num}cores_{main_core}mc.txt\")\n",
    "                            with open(path_errors + f\"bench_{bench_name}_{mem_resp_time}resp_{core_max_transactions}trans_{down_pending_max}dp_{probe_count}probes_{worker_num}cores_{main_core}mc.txt\", \"wb\") as binary_file:\n",
    "                                binary_file.write(stdout_data)\n",
    "                                \n",
    "                    except subprocess.TimeoutExpired:\n",
    "                        print(f\"    [ERROR] simulation timed out after {timeout} seconds\")\n",
    "                        \n",
    "                        if proc.pid:\n",
    "                            os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n",
    "            \n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "df = pd.DataFrame(data_lines, columns=data_header)\n",
    "df.to_csv(path_results + f\"/benchmark_{bench_name}_results.csv\", index=False)\n",
    "\n",
    "print(f\"Benchmark completed in {int(elapsed_time // 60)} min {round(elapsed_time % 60, 2)} s\")\n",
    "print(\"Result saved to \" + path_results + f\"/benchmark_{bench_name}_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
